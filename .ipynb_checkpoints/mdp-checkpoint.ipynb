{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690e6a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from transformers import ViTConfig\n",
    "from stable_baselines3 import PPO\n",
    "from typing import List\n",
    "\n",
    "class FixedViTMDPEnvironment(gym.Env):\n",
    "    \"\"\"Fixed ViT MDP environment for pruning.\"\"\"\n",
    "    def __init__(self, model_name=\"google/vit-base-patch16-224\", debug=True):\n",
    "        super().__init__()\n",
    "        self.debug = debug\n",
    "\n",
    "        self.config = ViTConfig.from_pretrained(model_name)\n",
    "        self.num_layers = self.config.num_hidden_layers\n",
    "        self.num_heads = self.config.num_attention_heads\n",
    "\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0.0, high=1.0, shape=(4,), dtype=np.float32\n",
    "        )\n",
    "        self.action_space = spaces.Box(\n",
    "            low=np.array([0.0, 0.0, 0.0], dtype=np.float32),\n",
    "            high=np.array([0.7, 0.7, 1.0], dtype=np.float32),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.current_layer = 0\n",
    "        self.total_reward = 0.0\n",
    "        self.layer_states = [\n",
    "            {\n",
    "                'heads_remaining': 1.0,\n",
    "                'ffn_size_ratio': 1.0,\n",
    "                'is_active': True,\n",
    "                'original_heads': self.num_heads,\n",
    "                'original_ffn_size': 3072\n",
    "            } for _ in range(self.num_layers)\n",
    "        ]\n",
    "        return self._get_current_state(), {}\n",
    "\n",
    "    def _get_current_state(self):\n",
    "        progress = self.current_layer / self.num_layers\n",
    "        avg_heads = np.mean([ls['heads_remaining'] for ls in self.layer_states])\n",
    "        avg_ffn = np.mean([ls['ffn_size_ratio'] for ls in self.layer_states])\n",
    "        active_ratio = sum(ls['is_active'] for ls in self.layer_states) / self.num_layers\n",
    "        return np.array([progress, avg_heads, avg_ffn, active_ratio], dtype=np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        head_ratio, ffn_ratio, skip_prob = map(float, action)\n",
    "        reward = self._apply_action(self.current_layer, head_ratio, ffn_ratio, skip_prob)\n",
    "        self.total_reward += reward\n",
    "        self.current_layer += 1\n",
    "        terminated = self.current_layer >= self.num_layers\n",
    "\n",
    "        if terminated:\n",
    "            reward += self._calculate_final_bonus()\n",
    "            self.total_reward += reward\n",
    "\n",
    "        next_state = self._get_current_state()\n",
    "        info = {\n",
    "            'layer': self.current_layer - 1,\n",
    "            'total_reward': self.total_reward,\n",
    "            'compression_ratio': self._calculate_compression_ratio(),\n",
    "            'active_layers': sum(ls['is_active'] for ls in self.layer_states)\n",
    "        }\n",
    "        return next_state, reward, terminated, False, info\n",
    "\n",
    "    def _apply_action(self, idx, head_prune, ffn_prune, skip_prob):\n",
    "        if idx >= len(self.layer_states):\n",
    "            return 0.0\n",
    "        state = self.layer_states[idx]\n",
    "        prev_heads = state['heads_remaining']\n",
    "        prev_ffn = state['ffn_size_ratio']\n",
    "        state['heads_remaining'] = max(0.1, prev_heads * (1 - head_prune))\n",
    "        state['ffn_size_ratio'] = max(0.1, prev_ffn * (1 - ffn_prune))\n",
    "        if skip_prob > 0.6:\n",
    "            state['is_active'] = False\n",
    "        return self._calculate_step_reward(idx, head_prune, ffn_prune, skip_prob, prev_heads, prev_ffn, state['is_active'])\n",
    "\n",
    "    def _calculate_step_reward(self, idx, head_prune, ffn_prune, skip_prob, prev_heads, prev_ffn, active):\n",
    "        head_comp = head_prune * prev_heads\n",
    "        ffn_comp = ffn_prune * prev_ffn\n",
    "        skip_comp = 1.0 if not active else 0.0\n",
    "        comp_reward = head_comp * 0.3 + ffn_comp * 0.5 + skip_comp * 1.0\n",
    "        importance = self._layer_importance(idx)\n",
    "        perf_penalty = head_prune * 0.4 * importance + ffn_prune * 0.6 * importance + skip_comp * 1.2 * importance\n",
    "        return 10.0 * (comp_reward - perf_penalty)\n",
    "\n",
    "    def _layer_importance(self, idx):\n",
    "        pos = idx / (self.num_layers - 1)\n",
    "        if pos < 0.3:\n",
    "            return 1.5\n",
    "        elif pos > 0.7:\n",
    "            return 1.3\n",
    "        return 1.0\n",
    "\n",
    "    def _calculate_final_bonus(self):\n",
    "        comp_ratio = self._calculate_compression_ratio()\n",
    "        active_count = sum(ls['is_active'] for ls in self.layer_states)\n",
    "        bonus = comp_ratio * 50.0\n",
    "        penalty = 20.0 if active_count < self.num_layers * 0.3 else 0.0\n",
    "        return bonus - penalty\n",
    "\n",
    "    def _calculate_compression_ratio(self):\n",
    "        total_orig = total_curr = 0.0\n",
    "        for ls in self.layer_states:\n",
    "            if ls['is_active']:\n",
    "                head_p = ls['original_heads'] * ls['heads_remaining'] * 64 * 768\n",
    "                ffn_p = ls['original_ffn_size'] * ls['ffn_size_ratio'] * 768\n",
    "                total_curr += head_p + ffn_p\n",
    "            total_orig += ls['original_heads'] * 64 * 768 + ls['original_ffn_size'] * 768\n",
    "        return max(0.0, min(1.0, 1.0 - (total_curr / total_orig)))\n",
    "\n",
    "def train_fixed_agent():\n",
    "    env = FixedViTMDPEnvironment(debug=False)\n",
    "    model = PPO(\"MlpPolicy\", env, verbose=1, n_steps=512, batch_size=32, n_epochs=5, learning_rate=3e-4)\n",
    "    model.learn(total_timesteps=2000)\n",
    "    for ep in range(3):\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        while not done:\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            obs, reward, done, _, info = env.step(action)\n",
    "            total_reward += reward\n",
    "        print(f\"Episode {ep+1}: Reward={info['total_reward']:.2f}, Compression={info['compression_ratio']:.2%}, Active Layers={info['active_layers']}/{env.num_layers}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_fixed_agent()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mdp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
